{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d353d4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import RepeatedKFold, KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91f91dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pymssql pandas imbalanced-learn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aef1cd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shree\\AppData\\Local\\Temp\\ipykernel_2836\\268454948.py:6: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Category  Rating_Count  Installs  Free  Size_in_Mb  Content_Rating  \\\n",
      "0   2.24681         262.0   10000.0     1    4.200000         2.18234   \n",
      "1   2.70581           8.0      10.0     1   36.000000         2.18234   \n",
      "2   2.45978        2352.0  500000.0     1   26.000000         2.18234   \n",
      "3   1.93159          70.0   10000.0     1    0.097656         2.18234   \n",
      "4   1.93159           0.0      10.0     1    2.900000         2.18234   \n",
      "\n",
      "   Ad_Supported  In_App_Purchases  Editors_Choice  Transformed_Rating  \n",
      "0             0                 1               0                   5  \n",
      "1             1                 0               0                   5  \n",
      "2             0                 0               0                   3  \n",
      "3             0                 0               0                   3  \n",
      "4             0                 0               0                   0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Connect to SQL Server\n",
    "conn = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"root\", database=\"GooglePlayStore\")\n",
    "\n",
    "# Fetch Data\n",
    "query = \"SELECT * FROM rating\"\n",
    "df = pd.read_sql(query, conn)\n",
    "\n",
    "# Close connection\n",
    "conn.close()\n",
    "\n",
    "df = df.drop('id', axis=1)\n",
    "# Show first few rows\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52668d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformed_Rating\n",
       "0    92553\n",
       "4    50957\n",
       "5    36943\n",
       "3    15689\n",
       "2     3489\n",
       "1      369\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Transformed_Rating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d3e34d",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2676efdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13d6a5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features Based on Decision Tree Importance:\n",
      "Index(['Category', 'Rating_Count', 'Size_in_Mb'], dtype='object')\n",
      "\n",
      "RFE Feature Rankings:\n",
      "Category: 1\n",
      "Content_Rating: 1\n",
      "Rating_Count: 1\n",
      "Size_in_Mb: 1\n",
      "Installs: 2\n",
      "Ad_Supported: 3\n",
      "In_App_Purchases: 4\n",
      "Free: 5\n",
      "Editors_Choice: 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "# Define target column\n",
    "target_col = \"Transformed_Rating\"\n",
    "X = df.drop(columns=[target_col])\n",
    "y = df[target_col]\n",
    "\n",
    "# Apply SMOTE\n",
    "# smote = SMOTE(sampling_strategy=\"auto\", random_state=7)\n",
    "# X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "\n",
    "# from imblearn.over_sampling import ADASYN\n",
    "# adasyn = ADASYN(random_state=42)\n",
    "# X_resampled, y_resampled = adasyn.fit_resample(X, y)\n",
    "\n",
    "# from imblearn.combine import SMOTETomek\n",
    "# smote_tomek = SMOTETomek(random_state=42)\n",
    "# X_resampled, y_resampled = smote_tomek.fit_resample(X, y)\n",
    "\n",
    "######################\n",
    "from imblearn.over_sampling import SMOTENC\n",
    "# Specify categorical feature indices (column positions)\n",
    "categorical_features = [0, 2]  # Adjust as per dataset\n",
    "smote_nc = SMOTENC(categorical_features=categorical_features, random_state=7)\n",
    "X_resampled, y_resampled = smote_nc.fit_resample(X, y)\n",
    "\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=7)\n",
    "\n",
    "\n",
    "# Train Decision Tree and Calculate Feature Importance\n",
    "dt = DecisionTreeClassifier(random_state=7)\n",
    "dt.fit(X_train, Y_train)\n",
    "\n",
    "# Mean importance threshold\n",
    "mean_imp = dt.feature_importances_.mean()\n",
    "selected_features = X_train.columns[dt.feature_importances_ > mean_imp]\n",
    "print(\"Selected Features Based on Decision Tree Importance:\")\n",
    "print(selected_features)\n",
    "\n",
    "# Recursive Feature Elimination (RFE)\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(random_state=7))\n",
    "rfe.fit(X_train, Y_train)\n",
    "\n",
    "# Print feature rankings\n",
    "print(\"\\nRFE Feature Rankings:\")\n",
    "for rank, feature in sorted(zip(rfe.ranking_, X_train.columns)):\n",
    "    print(f\"{feature}: {rank}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c055fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features Based on Decision Tree Importance:\n",
      "Index(['Rating_Count', 'Size_in_Mb'], dtype='object')\n",
      "\n",
      "RFE Feature Rankings:\n",
      "Category: 1\n",
      "Content_Rating: 1\n",
      "Rating_Count: 1\n",
      "Size_in_Mb: 1\n",
      "Installs: 2\n",
      "In_App_Purchases: 3\n",
      "Ad_Supported: 4\n",
      "Free: 5\n",
      "Editors_Choice: 6\n"
     ]
    }
   ],
   "source": [
    "# Feature and Target\n",
    "X = df.drop('Transformed_Rating', axis=1)\n",
    "Y = df['Transformed_Rating'].astype(int)  # Ensure categorical target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=7, stratify=Y)\n",
    "\n",
    "# Train Decision Tree and Calculate Feature Importance\n",
    "dt = DecisionTreeClassifier(random_state=7)\n",
    "dt.fit(X_train, Y_train)\n",
    "\n",
    "# Mean importance threshold\n",
    "mean_imp = dt.feature_importances_.mean()\n",
    "selected_features = X_train.columns[dt.feature_importances_ > mean_imp]\n",
    "print(\"Selected Features Based on Decision Tree Importance:\")\n",
    "print(selected_features)\n",
    "\n",
    "# Recursive Feature Elimination (RFE)\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(random_state=7))\n",
    "rfe.fit(X_train, Y_train)\n",
    "\n",
    "# Print feature rankings\n",
    "print(\"\\nRFE Feature Rankings:\")\n",
    "for rank, feature in sorted(zip(rfe.ranking_, X_train.columns)):\n",
    "    print(f\"{feature}: {rank}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e959360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution:\n",
      "Transformed_Rating\n",
      "0    92553\n",
      "4    50957\n",
      "5    36943\n",
      "3    15689\n",
      "2     3489\n",
      "1      369\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "target_col = \"Transformed_Rating\"\n",
    "class_counts = df[\"Transformed_Rating\"].value_counts()\n",
    "\n",
    "# Print class distribution\n",
    "print(\"Class Distribution:\")\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44c44b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed_Rating\n",
      "0    92553\n",
      "4    50957\n",
      "5    36943\n",
      "3    15689\n",
      "2     3489\n",
      "1      369\n",
      "Name: count, dtype: int64\n",
      "The data is imbalanced.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have a pandas DataFrame df with a target column 'target'\n",
    "# Replace 'target' with the actual column name of your target variable\n",
    "target_column = 'Transformed_Rating'\n",
    "\n",
    "# Check the distribution of the target variable\n",
    "class_counts = df[target_column].value_counts()\n",
    "\n",
    "# Print the distribution\n",
    "print(class_counts)\n",
    "\n",
    "# Check if the data is imbalanced\n",
    "# If the difference in class counts is large, it's likely imbalanced\n",
    "if class_counts.min() / class_counts.max() < 0.2:  # This is just an example threshold\n",
    "    print(\"The data is imbalanced.\")\n",
    "else:\n",
    "    print(\"The data is balanced.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a655838b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed_Rating\n",
      "0    369\n",
      "1    369\n",
      "2    369\n",
      "3    369\n",
      "4    369\n",
      "5    369\n",
      "Name: count, dtype: int64\n",
      "The data is imbalanced.\n"
     ]
    }
   ],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "# import pandas as pd\n",
    "\n",
    "# # Assuming your target column is 'target' and features are in 'X', with 'y' as the target\n",
    "# X = df.drop(columns=['Transformed_Rating'])  # Your feature matrix\n",
    "# y = df['Transformed_Rating']  # Your target variable\n",
    "\n",
    "# # Apply SMOTE to balance the dataset\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_res, y_res = smote.fit_resample(X, y)\n",
    "\n",
    "# from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# # Assuming your target column is 'target' and features are in 'X', with 'y' as the target\n",
    "# X = df.drop(columns=['Transformed_Rating'])  # Your feature matrix\n",
    "# y = df['Transformed_Rating']  # Your target variable\n",
    "\n",
    "# # Apply Random Undersampling to balance the dataset\n",
    "# under_sampler = RandomUnderSampler(random_state=42)\n",
    "# X_res, y_res = under_sampler.fit_resample(X, y)\n",
    "\n",
    "# # Check the new class distribution\n",
    "# print(pd.Series(y_res).value_counts())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Create stratified k-fold\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "\n",
    "# Check the new class distribution\n",
    "print(pd.Series(y_res).value_counts())\n",
    "if class_counts.min() / class_counts.max() < 0.2:  # This is just an example threshold\n",
    "    print(\"The data is imbalanced.\")\n",
    "else:\n",
    "    print(\"The data is balanced.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bde3660a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Visualizing the class distribution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "class_counts.plot(kind=\"bar\", color=[\"skyblue\", \"salmon\"])\n",
    "plt.title(\"Class Distribution\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "49689fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imbalance Ratio: 0.00\n"
     ]
    }
   ],
   "source": [
    "imbalance_ratio = class_counts.min() / class_counts.max()\n",
    "print(f\"Imbalance Ratio: {imbalance_ratio:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ddc00fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [160000, 140000]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HistGradientBoostingClassifier\n\u001b[0;32m      3\u001b[0m hgbc \u001b[38;5;241m=\u001b[39m HistGradientBoostingClassifier(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mhgbc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m hgbc\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m      6\u001b[0m hgbc_ac \u001b[38;5;241m=\u001b[39m accuracy_score(Y_test, y_pred)\n",
      "File \u001b[1;32mc:\\Users\\shree\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\shree\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\gradient_boosting.py:541\u001b[0m, in \u001b[0;36mBaseHistGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    539\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, estimator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    540\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_encode_y(y)\n\u001b[1;32m--> 541\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;66;03m# Do not create unit sample weights by default to later skip some\u001b[39;00m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;66;03m# computation\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\shree\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:475\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    473\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 475\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    476\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    477\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    478\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [160000, 140000]"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "hgbc = HistGradientBoostingClassifier(max_iter=100, learning_rate=0.1, random_state=7)\n",
    "hgbc.fit(X_train, Y_train)\n",
    "y_pred = hgbc.predict(X_test)\n",
    "hgbc_ac = accuracy_score(Y_test, y_pred)\n",
    "\n",
    "print(\"HistGradientBoosting Classifier Accuracy:\", hgbc_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3ff2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Model Logistic Regression\n",
    "\n",
    "logreg_c=LogisticRegression(max_iter=500, random_state=7, class_weight='balanced')\n",
    "logreg_c.fit(X_train,Y_train)\n",
    "logreg_pred=logreg_c.predict(X_test)\n",
    "logreg_cm=confusion_matrix(Y_test,logreg_pred)\n",
    "logreg_ac=accuracy_score(Y_test, logreg_pred)\n",
    "print('LogisticRegression_accuracy:',logreg_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c375a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Model RandomForest\n",
    "\n",
    "rdf_c=RandomForestClassifier(random_state=7)\n",
    "rdf_c.fit(X_train,Y_train)\n",
    "rdf_pred=rdf_c.predict(X_test)\n",
    "rdf_cm=confusion_matrix(Y_test,rdf_pred)\n",
    "rdf_ac=accuracy_score(rdf_pred,Y_test)\n",
    "print('RandomForest_Accuracy: ', rdf_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0e241c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Model DecisionTree Classifier\n",
    "\n",
    "dtree_c=DecisionTreeClassifier(random_state=7,criterion='entropy', max_depth = 10, min_samples_leaf = 2, min_samples_split = 5)\n",
    "dtree_c.fit(X_train,Y_train)\n",
    "dtree_pred=dtree_c.predict(X_test)\n",
    "dtree_cm=confusion_matrix(Y_test,dtree_pred)\n",
    "dtree_ac=accuracy_score(dtree_pred,Y_test)\n",
    "print('DecisionTreeClassifier_Accuracy: ',dtree_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b792da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Model Naive Bayesian\n",
    "\n",
    "NB = BernoulliNB(binarize = 0.0)\n",
    "NB.fit(X_train,Y_train)\n",
    "y_pred = NB.predict(X_test)\n",
    "nb_ac=accuracy_score(Y_test, y_pred)\n",
    "print(\"Bernoulli Naive Bayes_Accuracy: \", nb_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48f097f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning K-Nearest Neighbors (KNN)\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search_knn = GridSearchCV(knn, param_grid_knn, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_knn.fit(X_train, Y_train)\n",
    "\n",
    "# Best parameters and score for KNN\n",
    "print(\"Best Parameters (KNN):\", grid_search_knn.best_params_)\n",
    "print(\"Best Accuracy (KNN):\", grid_search_knn.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5894954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying K-Nearest Neighbors\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)  # You can change the number of neighbors\n",
    "knn.fit(X_train, Y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "knn_ac = accuracy_score(Y_test, y_pred)\n",
    "\n",
    "print(\"K-Nearest Neighbors Accuracy:\", knn_ac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792c5f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning Linear Discriminant Analysis (LDA)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid_lda = {\n",
    "    'solver': ['svd', 'lsqr', 'eigen']\n",
    "}\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "grid_search_lda = GridSearchCV(lda, param_grid_lda, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_lda.fit(X_train, Y_train)\n",
    "\n",
    "# Best parameters and score for LDA\n",
    "print(\"Best Parameters (LDA):\", grid_search_lda.best_params_)\n",
    "print(\"Best Accuracy (LDA):\", grid_search_lda.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85393a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8b4ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Linear Discriminant Analysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, Y_train)\n",
    "y_pred = lda.predict(X_test)\n",
    "lda_ac = accuracy_score(Y_test, y_pred)\n",
    "\n",
    "print(\"Linear Discriminant Analysis Accuracy:\", lda_ac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a6e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Gradient Boosting Classifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=7)\n",
    "gbc.fit(X_train, Y_train)\n",
    "y_pred = gbc.predict(X_test)\n",
    "gbc_ac = accuracy_score(Y_test, y_pred)\n",
    "\n",
    "print(\"Gradient Boosting Classifier Accuracy:\", gbc_ac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dd7d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying AdaBoost Classifier\n",
    "\n",
    "abc = AdaBoostClassifier(n_estimators=100, learning_rate=0.1, random_state=7)\n",
    "abc.fit(X_train, Y_train)\n",
    "y_pred = abc.predict(X_test)\n",
    "abc_ac = accuracy_score(Y_test, y_pred)\n",
    "\n",
    "print(\"AdaBoost Classifier Accuracy:\", abc_ac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce54cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning XGBoost Classifier\n",
    "# !pip install --upgrade xgboost scikit-learn\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 9]\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=7)\n",
    "grid_search_xgb = GridSearchCV(xgb, param_grid_xgb, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_xgb.fit(X_train, Y_train)\n",
    "\n",
    "# Best parameters and score for XGBoost\n",
    "print(\"Best Parameters (XGBoost):\", grid_search_xgb.best_params_)\n",
    "print(\"Best Accuracy (XGBoost):\", grid_search_xgb.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21175e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying XGBoost Classifier\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=7, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb.fit(X_train, Y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "xgb_ac = accuracy_score(Y_test, y_pred)\n",
    "\n",
    "print(\"XGBoost Classifier Accuracy:\", xgb_ac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470bbca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize and train the model\n",
    "hgb = HistGradientBoostingClassifier()\n",
    "hgb.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = hgb.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "hgb_ac = accuracy_score(Y_test, y_pred)\n",
    "print(\"HistGradientBoostingClassifier Accuracy:\", hgb_ac)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd32e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning CatBoost Classifier\n",
    "param_grid_catboost = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'depth': [4, 6, 8]\n",
    "}\n",
    "\n",
    "catboost = CatBoostClassifier(verbose=0)\n",
    "grid_search_catboost = GridSearchCV(catboost, param_grid_catboost, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_catboost.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score for CatBoost\n",
    "print(\"Best Parameters (CatBoost):\", grid_search_catboost.best_params_)\n",
    "print(\"Best Accuracy (CatBoost):\", grid_search_catboost.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf81031a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying Model CatBoost Model\n",
    "\n",
    "Cat_Boost = CatBoostClassifier(verbose = 0, n_estimators = 100)\n",
    "Cat_Boost.fit(X_train, Y_train)\n",
    "cb_ac=Cat_Boost.score(X_train, Y_train)\n",
    "print(\"CatBoost_Accuracy: \",cb_ac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d283ffc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b36be7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39df8e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import zscore\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "from sklearn.model_selection import RepeatedKFold, KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "\n",
    "# Load CSV into Pandas\n",
    "df = pd.read_csv(r\"C:\\Users\\shree\\Downloads\\Playstore\\sampled_file_processed_final.csv\")\n",
    "\n",
    "# Connect to MySQL\n",
    "conn = mysql.connector.connect(host=\"localhost\", user=\"root\", password=\"root\", database=\"GooglePlayStore\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Insert data row by row\n",
    "for _, row in df.iterrows():\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO rating (Category, Rating_Count, Installs, Free, Size_in_Mb, Content_Rating, Ad_Supported, In_App_Purchases, Editors_Choice, Transformed_Rating)\n",
    "        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\n",
    "    \"\"\", tuple(row))\n",
    "\n",
    "# Commit and close\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n",
    "\n",
    "print(\"CSV uploaded successfully!\")\n",
    "\n",
    "df.info()\n",
    "df.shape\n",
    "\n",
    "df.columns\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "missing_percentage = (df.isnull().sum().sort_values(ascending = False)/len(df))*100\n",
    "missing_percentage\n",
    "\n",
    "#Dropping unnecessary columns\n",
    "df=df.drop(columns=['App Name','App Id','Price','Currency','Minimum Installs','Maximum Installs','Developer Id','Developer Website','Minimum Android','Developer Email','Released','Last Updated','Privacy Policy','Scraped Time'])\n",
    "\n",
    "df['Category'].unique()\n",
    "\n",
    "df['Rating'].unique()\n",
    "\n",
    "df['Content Rating'].value_counts()\n",
    "\n",
    "df['Editors Choice'].unique()\n",
    "\n",
    "df.info()\n",
    "\n",
    "# # Calculate counts for exact values (e.g., 1.5, 2.5, etc.)\n",
    "counts = df['Rating'].value_counts()\n",
    "counts_dict = counts.to_dict()\n",
    "\n",
    "# Define a helper function to handle exact values (e.g., 1.5, 2.5, etc.)\n",
    "def handle_exact_rating(rating):\n",
    "    if rating in [1.5, 2.5, 3.5, 4.5]:  # Add more exact values if needed\n",
    "        higher_rating = round(rating + 0.1, 1)\n",
    "\n",
    "        higher_count = counts_dict.get(higher_rating, 0)\n",
    "        rating_count = counts_dict.get(rating, 0)\n",
    "\n",
    "        if rating < higher_count:\n",
    "            return int(rating)  # Round down\n",
    "        else:\n",
    "            return int(rating) + 1  # Round up\n",
    "    return None\n",
    "\n",
    "# Function to transform the Rating column\n",
    "def transform_rating(rating):\n",
    "    if pd.isnull(rating):\n",
    "        return np.nan\n",
    "    \n",
    "    if rating == 0:\n",
    "        return 0\n",
    "    elif 1 <= rating < 1.5:\n",
    "        return 1\n",
    "    elif 1.5 <= rating < 2.5:\n",
    "        return handle_exact_rating(1.5) if rating == 1.5 else 2\n",
    "    elif 2.5 <= rating < 3.5:\n",
    "        return handle_exact_rating(2.5) if rating == 2.5 else 3\n",
    "    elif 3.5 <= rating < 4.5:\n",
    "        return handle_exact_rating(3.5) if rating == 3.5 else 4\n",
    "    else:\n",
    "        return 5\n",
    "\n",
    "\n",
    "# Apply the transformation to the dataframe\n",
    "df['Transformed_Rating'] = df['Rating'].apply(transform_rating)\n",
    "\n",
    "df.drop('Rating',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "# Display the transformed dataframe\n",
    "df\n",
    "\n",
    "# Calculate the mean 'Rating' for each 'Category'\n",
    "category_means = df.groupby(\"Category\")[\"Transformed_Rating\"].mean()\n",
    "\n",
    "# Fill null or blank values in the 'Rating' column with the category-wise mean\n",
    "df[\"Transformed_Rating\"] = df.apply(\n",
    "    lambda row: category_means[row[\"Category\"]] if pd.isnull(row[\"Transformed_Rating\"]) else row[\"Transformed_Rating\"],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "df['Size'].unique()\n",
    "\n",
    "def convert_to_mb(size):\n",
    "    if isinstance(size, str):\n",
    "        # Remove commas from the string\n",
    "        size = size.replace(',', '')\n",
    "\n",
    "        if 'k' in size:\n",
    "            # Remove 'k' and convert to MB (1 MB = 1024 KB)\n",
    "            size = float(size.replace('k', '')) / 1024\n",
    "        elif 'M' in size:\n",
    "            # Remove 'M' (already in MB)\n",
    "            size = float(size.replace('M', ''))\n",
    "        elif 'G' in size:\n",
    "            # Remove 'G' and convert to MB (1GB = 1024 MB)\n",
    "            size = float(size.replace('G', '')) * 1024\n",
    "        elif 'Varies with device' in size:\n",
    "            return np.nan\n",
    "\n",
    "    return size\n",
    "\n",
    "df['Size'] = df['Size'].apply(convert_to_mb)\n",
    "# Rename the column 'Size' to 'Size_in_Mb'\n",
    "df.rename(columns={'Size': 'Size_in_Mb'}, inplace=True)\n",
    "\n",
    "# Repeat the process for Rating\n",
    "Q1 = df['Size_in_Mb'].quantile(0.25)\n",
    "Q3 = df['Size_in_Mb'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "outliers = df[(df['Size_in_Mb'] < lower_bound) | (df['Size_in_Mb'] > upper_bound)]\n",
    "print(\"\\nOutliers in Size column:\")\n",
    "print(outliers)\n",
    "\n",
    "#Replace null values in size column with median category wise \n",
    "#As mean values is greater than median and data is +vely skewed hence replacing with median \n",
    "df['Size_in_Mb'] = df['Size_in_Mb'].fillna(df.groupby('Category')['Size_in_Mb'].transform('median').round(2))\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "# Repeat the process for Rating\n",
    "Q1_rating_count = df['Rating Count'].quantile(0.25)\n",
    "Q3_rating_count = df['Rating Count'].quantile(0.75)\n",
    "IQR = Q3_rating_count - Q1_rating_count\n",
    "\n",
    "lower_bound = Q1_rating_count - 1.5 * IQR\n",
    "upper_bound = Q3_rating_count + 1.5 * IQR\n",
    "outliers = df[(df['Size_in_Mb'] < lower_bound) | (df['Size_in_Mb'] > upper_bound)]\n",
    "print(\"\\nOutliers in Size column:\")\n",
    "outliers\n",
    "\n",
    "df['Rating Count'] = df['Rating Count'].fillna(df.groupby('Category')['Rating Count'].transform('mean').round(2))\n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def k_fold_target_encoding(data, category_col, target_col, n_splits=5, smoothing=1):\n",
    "    global_mean = data[target_col].mean()\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    encoded_col = pd.Series(np.zeros(data.shape[0]), index=data.index)\n",
    "\n",
    "    for train_idx, val_idx in kf.split(data):\n",
    "        train_data, val_data = data.iloc[train_idx], data.iloc[val_idx]\n",
    "        \n",
    "        # Calculate category statistics on the training fold\n",
    "        category_stats = train_data.groupby(category_col)[target_col].agg(['mean', 'count'])\n",
    "        category_stats['smoothed'] = (category_stats['count'] * category_stats['mean'] + \n",
    "                                      smoothing * global_mean) / (category_stats['count'] + smoothing)\n",
    "        \n",
    "        # Map smoothed statistics to validation fold\n",
    "        val_data_encoded = val_data[category_col].map(category_stats['smoothed']).fillna(global_mean)\n",
    "        encoded_col.iloc[val_idx] = val_data_encoded\n",
    "\n",
    "    return encoded_col\n",
    "\n",
    "# Apply K-Fold Target Encoding and replace the original column\n",
    "df['Category'] = k_fold_target_encoding(df, category_col='Category', target_col='Transformed_Rating')\n",
    "\n",
    "# Apply K-Fold Target Encoding to 'Content Rating' column\n",
    "df['Content Rating'] = k_fold_target_encoding(df, category_col='Content Rating', target_col='Transformed_Rating')\n",
    "\n",
    "# Verify the result\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "df.columns\n",
    "\n",
    "#Cleaning Installs Column(remove , and + and convert the data type from object to float)\n",
    "df['Installs'] = df['Installs'].str.replace(',','').str.replace('+','').astype('float')\n",
    "\n",
    "df.shape\n",
    "\n",
    "df.head()\n",
    "\n",
    "# df_ohe = pd.get_dummies(df, columns=['Content Rating'])\n",
    "\n",
    "df.info()\n",
    "\n",
    "df.head()\n",
    "\n",
    "# boolean columns to 0 and 1\n",
    "bool_cols = ['Free', 'Ad Supported', 'In App Purchases','Editors Choice']\n",
    "df[bool_cols] = df[bool_cols].astype(int)\n",
    "\n",
    "df.head()\n",
    "\n",
    "df['Installs'] = df.groupby('Category')['Installs'].transform(lambda x: x.fillna(x.mean()))\n",
    "\n",
    "df.head()\n",
    "\n",
    "df_sampled = df.sample(n=200000, random_state=42)\n",
    "df_sampled.to_csv(\"sampled_file_processed_final.csv\", index=False)\n",
    "\n",
    "## Feature Extraction\n",
    "\n",
    "# Feature and Target\n",
    "X = df.drop('Transformed_Rating', axis=1)\n",
    "Y = df['Transformed_Rating'].astype(int)  # Ensure categorical target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=7, stratify=Y)\n",
    "\n",
    "# Train Decision Tree and Calculate Feature Importance\n",
    "dt = DecisionTreeClassifier(random_state=7)\n",
    "dt.fit(X_train, Y_train)\n",
    "\n",
    "# Mean importance threshold\n",
    "mean_imp = dt.feature_importances_.mean()\n",
    "selected_features = X_train.columns[dt.feature_importances_ > mean_imp]\n",
    "print(\"Selected Features Based on Decision Tree Importance:\")\n",
    "print(selected_features)\n",
    "\n",
    "# Recursive Feature Elimination (RFE)\n",
    "rfe = RFE(estimator=DecisionTreeClassifier(random_state=7))\n",
    "rfe.fit(X_train, Y_train)\n",
    "\n",
    "# Print feature rankings\n",
    "print(\"\\nRFE Feature Rankings:\")\n",
    "for rank, feature in sorted(zip(rfe.ranking_, X_train.columns)):\n",
    "    print(f\"{feature}: {rank}\")\n",
    "\n",
    "\n",
    "#Applying Model Logistic Regression\n",
    "\n",
    "logreg_c=LogisticRegression(max_iter=500, random_state=7, class_weight='balanced')\n",
    "logreg_c.fit(X_train,Y_train)\n",
    "logreg_pred=logreg_c.predict(X_test)\n",
    "logreg_cm=confusion_matrix(Y_test,logreg_pred)\n",
    "logreg_ac=accuracy_score(Y_test, logreg_pred)\n",
    "print('LogisticRegression_accuracy:',logreg_ac)\n",
    "\n",
    "#Applying Model RandomForest\n",
    "\n",
    "rdf_c=RandomForestClassifier(random_state=7)\n",
    "rdf_c.fit(X_train,Y_train)\n",
    "rdf_pred=rdf_c.predict(X_test)\n",
    "rdf_cm=confusion_matrix(Y_test,rdf_pred)\n",
    "rdf_ac=accuracy_score(rdf_pred,Y_test)\n",
    "print('RandomForest_Accuracy: ', rdf_ac)\n",
    "\n",
    "# Fine-tuning Decision Tree Classifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "param_grid_dtree = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "dtree = DecisionTreeClassifier(random_state=7)\n",
    "grid_search_dtree = GridSearchCV(dtree, param_grid_dtree, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_dtree.fit(X_train, Y_train)\n",
    "\n",
    "# Best parameters and score for Decision Tree\n",
    "print(\"Best Parameters (Decision Tree):\", grid_search_dtree.best_params_)\n",
    "print(\"Best Accuracy (Decision Tree):\", grid_search_dtree.best_score_)\n",
    "\n",
    "#Applying Model DecisionTree Classifier\n",
    "\n",
    "dtree_c=DecisionTreeClassifier(random_state=7,)\n",
    "dtree_c.fit(X_train,Y_train)\n",
    "dtree_pred=dtree_c.predict(X_test)\n",
    "dtree_cm=confusion_matrix(Y_test,dtree_pred)\n",
    "dtree_ac=accuracy_score(dtree_pred,Y_test)\n",
    "print('DecisionTreeClassifier_Accuracy: ',dtree_ac)\n",
    "\n",
    "#Applying Model Naive Bayesian\n",
    "\n",
    "NB = BernoulliNB(binarize = 0.0)\n",
    "NB.fit(X_train,Y_train)\n",
    "y_pred = NB.predict(X_test)\n",
    "nb_ac=accuracy_score(Y_test, y_pred)\n",
    "print(\"Bernoulli Naive Bayes_Accuracy: \", nb_ac)\n",
    "\n",
    "# Fine-tuning K-Nearest Neighbors (KNN)\n",
    "param_grid_knn = {\n",
    "    'n_neighbors': [3, 5, 7, 9],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "grid_search_knn = GridSearchCV(knn, param_grid_knn, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_knn.fit(X_train, Y_train)\n",
    "\n",
    "# Best parameters and score for KNN\n",
    "print(\"Best Parameters (KNN):\", grid_search_knn.best_params_)\n",
    "print(\"Best Accuracy (KNN):\", grid_search_knn.best_score_)\n",
    "\n",
    "# Applying K-Nearest Neighbors\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5)  # You can change the number of neighbors\n",
    "knn.fit(X_train, Y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "knn_ac = accuracy_score(Y_test, y_pred)\n",
    "\n",
    "print(\"K-Nearest Neighbors Accuracy:\", knn_ac)\n",
    "\n",
    "\n",
    "# Fine-tuning Linear Discriminant Analysis (LDA)\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid_lda = {\n",
    "    'solver': ['svd', 'lsqr', 'eigen']\n",
    "}\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "grid_search_lda = GridSearchCV(lda, param_grid_lda, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_lda.fit(X_train, Y_train)\n",
    "\n",
    "# Best parameters and score for LDA\n",
    "print(\"Best Parameters (LDA):\", grid_search_lda.best_params_)\n",
    "print(\"Best Accuracy (LDA):\", grid_search_lda.best_score_)\n",
    "\n",
    "\n",
    "\n",
    "# Applying Linear Discriminant Analysis\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "lda.fit(X_train, Y_train)\n",
    "y_pred = lda.predict(X_test)\n",
    "lda_ac = accuracy_score(Y_test, y_pred)\n",
    "\n",
    "print(\"Linear Discriminant Analysis Accuracy:\", lda_ac)\n",
    "\n",
    "\n",
    "# Applying Gradient Boosting Classifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=7)\n",
    "gbc.fit(X_train, Y_train)\n",
    "y_pred = gbc.predict(X_test)\n",
    "gbc_ac = accuracy_score(Y_test, y_pred)\n",
    "\n",
    "print(\"Gradient Boosting Classifier Accuracy:\", gbc_ac)\n",
    "\n",
    "\n",
    "# Applying AdaBoost Classifier\n",
    "\n",
    "abc = AdaBoostClassifier(n_estimators=100, learning_rate=0.1, random_state=7)\n",
    "abc.fit(X_train, Y_train)\n",
    "y_pred = abc.predict(X_test)\n",
    "abc_ac = accuracy_score(Y_test, y_pred)\n",
    "\n",
    "print(\"AdaBoost Classifier Accuracy:\", abc_ac)\n",
    "\n",
    "\n",
    "# Fine-tuning XGBoost Classifier\n",
    "# !pip install --upgrade xgboost scikit-learn\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 9]\n",
    "}\n",
    "\n",
    "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=7)\n",
    "grid_search_xgb = GridSearchCV(xgb, param_grid_xgb, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_xgb.fit(X_train, Y_train)\n",
    "\n",
    "# Best parameters and score for XGBoost\n",
    "print(\"Best Parameters (XGBoost):\", grid_search_xgb.best_params_)\n",
    "print(\"Best Accuracy (XGBoost):\", grid_search_xgb.best_score_)\n",
    "\n",
    "\n",
    "# Applying XGBoost Classifier\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=7, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb.fit(X_train, Y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "xgb_ac = accuracy_score(Y_test, y_pred)\n",
    "\n",
    "print(\"XGBoost Classifier Accuracy:\", xgb_ac)\n",
    "\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize and train the model\n",
    "hgb = HistGradientBoostingClassifier()\n",
    "hgb.fit(X_train, Y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = hgb.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "hgb_ac = accuracy_score(Y_test, y_pred)\n",
    "print(\"HistGradientBoostingClassifier Accuracy:\", hgb_ac)\n",
    "\n",
    "\n",
    "# Fine-tuning CatBoost Classifier\n",
    "param_grid_catboost = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'depth': [4, 6, 8]\n",
    "}\n",
    "\n",
    "catboost = CatBoostClassifier(verbose=0)\n",
    "grid_search_catboost = GridSearchCV(catboost, param_grid_catboost, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search_catboost.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score for CatBoost\n",
    "print(\"Best Parameters (CatBoost):\", grid_search_catboost.best_params_)\n",
    "print(\"Best Accuracy (CatBoost):\", grid_search_catboost.best_score_)\n",
    "\n",
    "#Applying Model CatBoost Model\n",
    "\n",
    "Cat_Boost = CatBoostClassifier(verbose = 0, n_estimators = 100)\n",
    "Cat_Boost.fit(X_train, Y_train)\n",
    "cb_ac=Cat_Boost.score(X_train, Y_train)\n",
    "print(\"CatBoost_Accuracy: \",cb_ac)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
